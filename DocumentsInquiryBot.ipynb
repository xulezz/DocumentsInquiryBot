{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8bqW40_GhAD"
      },
      "outputs": [],
      "source": [
        "!pip install langchain[llms]\n",
        "!pip install -U langchain-community\n",
        "!pip install --upgrade --quiet docx2txt\n",
        "!pip install --quiet langchain_experimental langchain_openai\n",
        "!pip install faiss-cpu\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install pypdf\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1sDUOKAeG_2",
        "outputId": "7df44524-d42c-429e-fe74-ab5f99aca940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rco314bifhgJ",
        "outputId": "53f9649c-ce3d-4297-e846-1967ffefc2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgPRNpcgeIAU"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi7tmtuWfpRU"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for file in os.listdir(\"/content/drive/MyDrive/ColabNotebooks/docs/\"):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        pdf_path = \"/content/drive/MyDrive/ColabNotebooks/docs/\" + file\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents.extend(loader.load())\n",
        "    elif file.endswith('.txt'):\n",
        "        text_path = \"/content/drive/MyDrive/ColabNotebooks/docs/\" + file\n",
        "        loader = TextLoader(text_path)\n",
        "        documents.extend(loader.load())\n",
        "    elif file.endswith('.docx'):\n",
        "        docx_path = \"/content/drive/MyDrive/ColabNotebooks/docs/\" + file\n",
        "        loader = Docx2txtLoader(docx_path)\n",
        "        documents.extend(loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fGu8dRogpJf"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Text Splitter\n",
        "\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
        "documents = text_splitter.create_documents([d.page_content for d in documents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ueI_QjFbj8x"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# vectorDB usando OpenAIEmbeddings tranformer y guardamos en directorio de data\n",
        "vectordb = Chroma.from_documents(\n",
        "  documents,\n",
        "  embedding=OpenAIEmbeddings(),\n",
        "  persist_directory='./data'\n",
        ")\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXqFvlTLhXDC"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(model_name='gpt-4o'),\n",
        "    retriever=vectordb.as_retriever(search_kwargs={'k': 6}),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# testeamos que ya podemos tirarle queries contra nuestros documentos\n",
        "result = qa_chain({'query': 'What is the protocol number?'})\n",
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B9vP2ogtQlI"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import SystemMessagePromptTemplate\n",
        "from langchain.prompts.chat import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "general_system_template = r\"\"\"\n",
        "Use the following pieces of context to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "----\n",
        "{context}\n",
        "----\n",
        "\"\"\"\n",
        "general_user_template = \"Question:```{question}```\"\n",
        "messages = [\n",
        "            SystemMessagePromptTemplate.from_template(general_system_template),\n",
        "            HumanMessagePromptTemplate.from_template(general_user_template)\n",
        "]\n",
        "qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    return_messages=True,\n",
        "    output_key='answer'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HheGXPN2iTJw"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(temperature=0, model_name=\"gpt-4o\"),\n",
        "    vectordb.as_retriever(search_kwargs={'k': 12}),\n",
        "    return_source_documents=True,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    combine_docs_chain_kwargs={'prompt': qa_prompt}, # acá le pasamos nuestro prompt en específico para no recaer en el prompt default de ConversationalRetrievalChain / Igualmente lo mantuvimos parecido para no experimentar demasiado.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ks42KeAgiGKj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "\n",
        "    query = input('Prompt: ')\n",
        "\n",
        "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
        "        print('Exiting')\n",
        "        sys.exit()\n",
        "\n",
        "    result = chain({'question': query,\n",
        "                    'chat_history': chat_history\n",
        "                    })\n",
        "    print('Answer: ' + result['answer'])\n",
        "\n",
        "    chat_history.append((query, result['answer']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwcnw5TnW5mw"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}